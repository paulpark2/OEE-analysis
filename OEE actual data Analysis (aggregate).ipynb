{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP\tSolder die attachLSD003, LSD004, LSD005, LSD006, LSD007, LSD009, LSD010, LSD013, LSD014, LSD015, LSD016, LSD018, LSD019, LSD024, LSD025, LSD026, LSD036, LSD037\n",
    "FP\tAL wire bonding\tLAW004, LAW006, LAW007, LAW008, LAW009, LAW010, LAW011, LAW012, LAW013, LAW016, LAW018, LAW020, LAW022, LAW024, LAW026, LAW030, LAW033, LAW034, LAW035, LAW036, LAW040, LAW043, LAW050\n",
    "FP\tMold - ASM\tLMD002, LMD003, LMD004, LMD005, LMD006\n",
    "FP\tMold - Hanmi\tLMD007, LMD011\n",
    "DCB\tSolder die attach\tLSD001, LSD002, LSD011,LSD017, LSD008, LSD012, LSD011, LSD017, LSD022, LSD023, LSD027, LSD028, LSD029, LSD035, LSD038, LSD039, LSD040, LSD042\n",
    "DCB\tAL wire bonding - Power & gate\tLAW028, LAW029, LAW032, LAW038, LAW045, LAW046, LAW053\n",
    "DCB\tAL wire bonding - Signal\tLAW014, LAW037, LAW042, LAW044, LAW052\n",
    "DCB\tMold - Hanmi\tLMD009, LMD011, LMD012\n",
    "Mini Common\tPCB singulation\tLPS002\n",
    "Mini Common\tPCB connection\tLPC001, LPC002\n",
    "Mini Common\tPCB singualtion & connection\tLPC003, LPC004, LPC005\n",
    "Mini Common\tEpoxy die attach\tLED001, LED002, LED003, LED004, LED005, LED006, LED007, LED008, LED011, LED012, LED013\n",
    "Mini Common\tAu wire bonding\tLGW001, LGW002, LGW003, LGW004, LGW005, LGW006, LGW007, LGW008, LGW009, LGW010, LGW012, LGW013, LGW014, LGW015, LGW018, LGW019, LGW020, LGW021\n",
    "Mini Common\tIVI\tAIVI001, AIVI002, AIVI003, AIVI004, AIVI005, AIVI006\n",
    "Mini Common\tLaser marking\tLMK001, LMK002, LMK003, LMK005\n",
    "Mini Common\tTrim & form\tLTF001, LTF002, LTF003\n",
    "Mini Common\tTest Handler system\tLTE002, LTE004, LTE005, LTE006, LTE007, LTE010, LTE011, LTE012, LTE013, LTE014\n",
    "Mini Common\tAFVI\tAFVI001, AFVI002\n",
    "Tiny\tSolder die attach\tLSD020, LSD021, LSD030, LSD031\n",
    "Tiny\tEpoxy die attach\tLED009, LED010\n",
    "Tiny\tPotting\tLED015, LED018, LED019\n",
    "Tiny\tAl wire bonding -1x head\tLAW041, LAW047\n",
    "Tiny\tAl wire bonding -3x head\tLAW031, LAW039, LAW049\n",
    "Tiny\tPlasma - Au WB\tLPL002\n",
    "Tiny\tAu wire bonding\tLGW011, LGW016, LGW017\n",
    "Tiny\tIVI\tAIVI007, AIVI008\n",
    "Tiny\tPlasma - Mold\tLPL003\n",
    "Tiny\tMold - ASM\tLMD010, LMD013\n",
    "Tiny\tLaser marking\tLMK004\n",
    "Tiny\tTrim & form\tLTF005\n",
    "Tiny\tLeadcoversion (Tiny)\tLLC001, LLC002\n",
    "Tiny\tTest Handler system\tLTE008, LTE009\n",
    "Tiny\tAFVI\tAFVI003\n",
    "Tiny 3.0\tSolder die attach\tLSD041\n",
    "Tiny 3.0\tPCB singulation\tLPS002\n",
    "Tiny 3.0\tPCB connection\tLPC007\n",
    "Tiny 3.0\tEpoxy die attach\tLED020, LED014\n",
    "Tiny 3.0\tAL wire bonding - Power & gate\tLAW048\n",
    "Tiny 3.0\tAL wire bonding - Signal\tLAW051\n",
    "Tiny 3.0\tAu wire bonding\tLGW023\n",
    "Tiny 3.0\tIVI\tAIVI009\n",
    "Tiny 3.0\tPlasma - Mold\tLPL004\n",
    "Tiny 3.0\tMold - ASM\tLMD014\n",
    "Tiny 3.0\tLaser marking\tLMK004\n",
    "Tiny 3.0\tTrim & form\tLTF008\n",
    "Tiny 3.0\tTest Handler system\tLTE015\n",
    "Tiny 3.0\tAFVI\tAFVI003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PFC0 = ['LSD001','LSD012','LSD012','LSD023','LSD028','LSD035','LSD039','LSD040']\n",
    "PFC4 = ['LSD001','LSD012','LSD023','LSD027','LSD035','LSD039','LSD040']\n",
    "PFC8 = ['LSD001','LSD012','LSD023','LSD027','LSD035','LSD038']\n",
    "\n",
    "<All SDA>\n",
    "'LSD001', 'LSD002', 'LSD003', 'LSD004', 'LSD005', 'LSD006', 'LSD007', 'LSD008', 'LSD009', 'LSD010', 'LSD011', 'LSD012', 'LSD013', 'LSD014', 'LSD015', 'LSD016', 'LSD017', 'LSD018', 'LSD019', 'LSD020', 'LSD021', 'LSD022', 'LSD023', 'LSD024', 'LSD025', 'LSD026', 'LSD027', 'LSD028', 'LSD029', 'LSD030', 'LSD031', 'LSD035', 'LSD036', 'LSD037', 'LSD038', 'LSD039', 'LSD040', 'LSD041', 'LSD042'\n",
    "\n",
    "<BN SDA>\n",
    "'LSD003', 'LSD004', 'LSD005', 'LSD006', 'LSD007', 'LSD009', 'LSD010', 'LSD013', 'LSD014', 'LSD015', 'LSD016', 'LSD018', 'LSD019', 'LSD024', 'LSD025', 'LSD026', 'LSD036', 'LSD037','LSD001', 'LSD011', 'LSD012', 'LSD017', 'LSD023', 'LSD027', 'LSD028', 'LSD029', 'LSD035', 'LSD038', 'LSD039', 'LSD040','LSD020', 'LSD021', 'LSD030', 'LSD031','LSD041'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PFC0 = ['LSD002','LSD011','LSD027','LSD022','LSD008','LSD038','LSD029','LSD042']\n",
    "PFC4 = ['LSD002','LSD011','LSD017','LSD028','LSD022','LSD008','LSD038','LSD029','LSD042']\n",
    "PFC8 = ['LSD002','LSD011','LSD017','LSD028','LSD022','LSD008','LSD040','LSD029','LSD039','LSD042']\n",
    "df = df[~(df['Month'].isin(['2022-03']) & df['EQ_ID'].isin(PFC4))]\n",
    "df = df[~(df['LOG_WEEK'].isin(['LW-2023-06']) & df['EQ_ID'].isin(PFC4))]\n",
    "df = df[~(df['LOG_WEEK'].isin(['LW-2023-07']) & df['EQ_ID'].isin(PFC4))]\n",
    "df = df[~(df['LOG_WEEK'].isin(['LW-2023-08']) & df['EQ_ID'].isin(PFC0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import openpyxl\n",
    "from openpyxl.utils import range_boundaries\n",
    "import os\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [\n",
    "    #pd.read_csv('202010-202105OEE.csv'),\n",
    "    #pd.read_csv('202106-202111OEE.csv'),\n",
    "    #pd.read_csv('202112-2201OEE.csv'),\n",
    "    #pd.read_csv('202112OEE.csv'),\n",
    "    #pd.read_csv('202201OEE.csv'),\n",
    "    #pd.read_csv('202202OEE.csv'),\n",
    "    #pd.read_csv('202203OEE.csv'),\n",
    "    #pd.read_csv('202204OEE.csv'),\n",
    "    #pd.read_csv('202205OEE.csv'),\n",
    "    #pd.read_csv('202206OEE.csv'),\n",
    "    pd.read_csv('202207OEE.csv'),\n",
    "    pd.read_csv('202208OEE.csv'),\n",
    "    pd.read_csv('202209OEE.csv'),\n",
    "    pd.read_csv('202210OEE.csv'),\n",
    "    pd.read_csv('202211OEE.csv'),\n",
    "    pd.read_csv('202212OEE.csv'),\n",
    "    pd.read_csv('202301OEE.csv'),\n",
    "    pd.read_csv('202302OEE.csv'),\n",
    "    pd.read_csv('202303OEE.csv')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['LOG_WEEK', 'LOG_DATE', 'EQUIPMENT_NAME', 'LEVEL5_NAME', 'LEVEL6_NAME', 'DURATION_HOUR','PACKAGE_CLASS_PD', 'PPOS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'EQUIPMENT_NAME' : 'EQ_ID'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.rename(columns = {'Log Week' : 'LOG_WEEK', 'Fiscal Date' : 'LOG_DATE', 'Equipment Name' : 'EQ_ID', 'Level5 Name' : 'LEVEL5_NAME','Level6 Name' : 'LEVEL6_NAME', 'Duration Hour' : 'DURATION_HOUR'}, inplace = True)\n",
    "#df0.rename(columns = {'Log Week' : 'LOG_WEEK', 'Fiscal Date' : 'LOG_DATE', 'Equipment Name' : 'EQ_ID', 'Level5 Name' : 'LEVEL5_NAME','Level6 Name' : 'LEVEL6_NAME', 'Duration Hour' : 'DURATION_HOUR'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.LEVEL5_NAME != 'NO MATERIAL')]\n",
    "#df = df[(df.LEVEL5_NAME != 'SPEED LOSS')]\n",
    "df = df[(df.LEVEL5_NAME != 'INVALID DATA')]\n",
    "df = df[(df.LEVEL5_NAME != 'EXCEPTION')]\n",
    "df = df[(df.LEVEL5_NAME != 'NO COMMUNICATION')]\n",
    "df = df[(df.LEVEL5_NAME != 'NON-SCHEDULED TIME')]\n",
    "#df = df[(df.LEVEL5_NAME != 'ENGINEERING EVALUATION')]\n",
    "#df = df[(df.LEVEL5_NAME != '<unrecognized>')]\n",
    "df = df[(df.EQ_ID != 'LSD999')]\n",
    "df = df[(df.EQ_ID != 'LAW099')]\n",
    "df = df[(df.EQ_ID != 'LSD099')]\n",
    "df = df[(df.EQ_ID != 'AFVI099')]\n",
    "df = df[(df.LOG_DATE != '2022-08-29') | (df.EQ_ID != 'LAW004') | (df.LEVEL6_NAME != 'WAITING FOR RESPONSE')]\n",
    "df = df[(df.LOG_DATE != '2022-08-27') | (df.EQ_ID != 'LAW004') | (df.LEVEL6_NAME != 'WAITING FOR RESPONSE')]\n",
    "df = df[(df.LOG_DATE != '2022-08-28') | (df.EQ_ID != 'LAW004') | (df.LEVEL6_NAME != 'WAITING FOR RESPONSE')]\n",
    "#LAW004 2022-08-27  5:48:00 오전 ~ 2022-08-29  8:42:00 오전 Bitlocker  (Waiting for response)\n",
    "df = df[(df.LOG_DATE != '2022-08-28') | (df.EQ_ID != 'LMD012') | (df.LEVEL6_NAME != 'WAITING FOR RESPONSE') | (df.PACKAGE_CLASS_PD != 'MDIP-DCB')]\n",
    "df = df[(df.LOG_DATE != '2022-08-29') | (df.EQ_ID != 'LMD012') | (df.LEVEL6_NAME != 'WAITING FOR RESPONSE') | (df.PACKAGE_CLASS_PD != 'MDIP-DCB')]\n",
    "#LMD012 2022-08-28  4:17:00 오전 ~ 2022-08-29  8:45:00 오전 Bitlocker  (Waiting for response)\n",
    "df.drop(df.loc[(df.LOG_DATE == '2022-10-22') & (df.EQ_ID == 'LMD011') & (df.LEVEL6_NAME == 'WAITING FOR RESPONSE') & (df.DURATION_HOUR > 4)].index,inplace=True)\n",
    "#LMD011 2022-10-22 9:35:00 오전 ~ 2022-10-22  6:25:00 오후 miss key-in  (Waiting for response)\n",
    "df = df[(df.LOG_DATE != '2022-10-25') | (df.EQ_ID != 'LSD018') | (df.LEVEL6_NAME != 'CHANGE COLLET')]\n",
    "df = df[(df.LOG_DATE != '2022-10-25') | (df.EQ_ID != 'LSD019') | (df.LEVEL6_NAME != 'CHANGE COLLET')]\n",
    "df = df[(df.LOG_DATE != '2022-10-18') | (df.EQ_ID != 'LSD007') | (df.LEVEL6_NAME != 'CHANGE COLLET')]\n",
    "df = df[(df.LOG_DATE != '2022-10-18') | (df.EQ_ID != 'LSD006') | (df.LEVEL6_NAME != 'CHANGE COLLET')]\n",
    "\n",
    "mask = (df.LOG_DATE == '2023-02-13') & (df.EQ_ID == 'LMD004') & (df.LEVEL6_NAME == 'DAILY, WEEKLY')\n",
    "df.loc[mask, 'LEVEL5_NAME'] = 'MACHINE FAILURE'\n",
    "df.loc[mask, 'LEVEL6_NAME'] = 'INPUT HANDLER PROBLEM'\n",
    "mask2 = (df.LOG_DATE == '2023-02-15') & (df.EQ_ID == 'LMD004') & (df.LEVEL6_NAME == 'DAILY, WEEKLY')\n",
    "df.loc[mask2, 'LEVEL5_NAME'] = 'MACHINE FAILURE'\n",
    "df.loc[mask2, 'LEVEL6_NAME'] = 'PELLET PROBLEM'\n",
    "\n",
    "#df = df[(df.EQ_ID != 'LSD032')]\n",
    "#df = df[(df.EQ_ID != 'LSD033')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Process'] = df['EQ_ID'].str.extract(pat = '([A-Z]..)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Month'] = pd.to_datetime(df['LOG_DATE']).dt.to_period('M') #.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df['PACKAGE_CLASS_PD'].isin(['MDIP-FP', 'MDIP-DCB']), 'EQ_ID'] = 'new_value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PFC bottleneck 기준 data 뽑기\n",
    "PFC_D = ['IM564-X6D','IFCM30T65GD','IFCM20T65GD', 'IFCM15S60GS', 'IFCM15P60GD', 'IFCM15S60GD']\n",
    "PFC_EQ = ['LSD011', 'LSD017','LSD028', 'LSD029', 'LSD040', 'LSD039']\n",
    "PFC_EQ2 = ['LSD011', 'LSD027','LSD029', 'LSD038']\n",
    "df.sort_values(by=['EQ_ID','LOG_DATE'], inplace=True)\n",
    "df['PPOS'] = np.where((df.Month >= '2021-12'), df['PPOS'].fillna(method = 'bfill'), df['PPOS'])\n",
    "df['PPOS'] = np.where((df.Month >= '2021-12'), df['PPOS'].fillna(method = 'ffill'), df['PPOS'])\n",
    "\n",
    "df = df[~( (df['PPOS'].isin(PFC_D)) & (df['EQ_ID'].isin(PFC_EQ)) & (df.Month >= '2021-12')) ]\n",
    "df = df[~( (~df['PPOS'].isin(PFC_D)) & (df['EQ_ID'].isin(PFC_EQ2)) & (df.Month >= '2021-12')) ]\n",
    "\n",
    "#df = df[~(np.logical_not(df['PPOS'].isin(PFC_D)) & df['EQ_ID'].isin(PFC_EQ2) & df['Month'].isin(['2022-10','2022-11']) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df['PACKAGE_CLASS_PD'] == 'MDIP-FP', 'EQ_ID'] = 'new_value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq = pd.read_csv('EQ_Process_BN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq['EQ_ID'] = pq['EQ_ID'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv('OEE Target3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target['Level 5'] = target['Level 5'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['week'] = pd.to_datetime(df['LOG_DATE']).dt.to_period('Y').astype(str) + '_' + df['LOG_WEEK'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = {\n",
    "    #BN SDA\n",
    "    #'a' : ['LSD003', 'LSD004', 'LSD005', 'LSD006', 'LSD007', 'LSD009', 'LSD010', 'LSD013', 'LSD014', 'LSD015', 'LSD016', 'LSD018', 'LSD019', 'LSD024', 'LSD025', 'LSD026', 'LSD036', 'LSD037','LSD001', 'LSD011', 'LSD012', 'LSD017', 'LSD023', 'LSD027', 'LSD028', 'LSD029', 'LSD035', 'LSD038', 'LSD039', 'LSD040','LSD020', 'LSD021', 'LSD030', 'LSD031','LSD041']\n",
    "    'b' : ['LMD002', 'LMD003', 'LMD004', 'LMD005', 'LMD006', 'LMD007', 'LMD009', 'LMD010', 'LMD011', 'LMD012', 'LMD013', 'LMD14']\n",
    "    #'a' : ['LAW004', 'LAW006', 'LAW007', 'LAW008', 'LAW009', 'LAW010', 'LAW011', 'LAW012', 'LAW013', 'LAW016', 'LAW018', 'LAW020', 'LAW022', 'LAW024', 'LAW026', 'LAW030', 'LAW033', 'LAW034', 'LAW035', 'LAW036', 'LAW040', 'LAW043', 'LAW050']\n",
    "    #'b' : ['LSD001', 'LSD012', 'LSD023', 'LSD027', 'LSD035', 'LSD038']\n",
    "    #'c': ['LAW004','LAW005', 'LAW006', 'LAW007', 'LAW008', 'LAW009', 'LAW010', 'LAW011', 'LAW012', 'LAW013', 'LAW016', 'LAW018', 'LAW020', 'LAW022', 'LAW024', 'LAW026', 'LAW030', 'LAW033', 'LAW034', 'LAW035', 'LAW036', 'LAW040', 'LAW043']\n",
    "    #'d': ['LSD001', 'LSD002','LSD008','LSD011', 'LSD012', 'LSD017', 'LSD022','LSD023', 'LSD027', 'LSD028', 'LSD029', 'LSD035', 'LSD038', 'LSD039', 'LSD040','LSD042']\n",
    "    #'e':['LSD001', 'LSD003', 'LSD004', 'LSD005', 'LSD006', 'LSD007', 'LSD009', 'LSD010', 'LSD012', 'LSD013', 'LSD014', 'LSD015', 'LSD016', 'LSD018', 'LSD019', 'LSD020', 'LSD021', 'LSD023', 'LSD024', 'LSD025', 'LSD026', 'LSD027', 'LSD030', 'LSD031', 'LSD035', 'LSD036', 'LSD037', 'LSD038', 'LSD041']\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly(com, df, pq, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly(com, df, pq, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily(com, df, pq, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.crosstab([df2.LEVEL5_NAME,df2.LEVEL6_NAME],df2.Month,values=df2.DURATION_HOUR,aggfunc=sum,rownames=['Level 5','LV6'],colnames=['month'], normalize='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.to_excel(\"ch.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL(df, pq, target) # LV6 of all process and each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_page(df, pq, target) # LV5 OF all process and one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['LSD003', 'LSD004', 'LSD005', 'LSD006', 'LSD007', 'LSD009', 'LSD010', 'LSD013', 'LSD014', 'LSD015', 'LSD016', 'LSD018', 'LSD019', 'LSD024', 'LSD025', 'LSD026', 'LSD036', 'LSD037','LSD001', 'LSD011', 'LSD012', 'LSD017', 'LSD023', 'LSD027', 'LSD028', 'LSD029', 'LSD035', 'LSD038', 'LSD039', 'LSD040','LSD020', 'LSD021', 'LSD030', 'LSD031','LSD041']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['LSD003', 'LSD004', 'LSD005', 'LSD006', 'LSD007', 'LSD009', 'LSD010', 'LSD013', 'LSD014', 'LSD015', 'LSD016', 'LSD018', 'LSD019', 'LSD024', 'LSD025', 'LSD026', 'LSD036', 'LSD037','LSD001', 'LSD011', 'LSD012', 'LSD017', 'LSD023', 'LSD027', 'LSD028', 'LSD029', 'LSD035', 'LSD038', 'LSD039', 'LSD040','LSD020', 'LSD021', 'LSD030', 'LSD031','LSD041']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1 = df['EQ_ID'].isin(lst)\n",
    "df2 = df[filter1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[(df2.Month == '2023-03')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parkpaul\\AppData\\Local\\Temp\\ipykernel_18712\\3738910620.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append(p.transpose())\n"
     ]
    }
   ],
   "source": [
    "EQ_ID(df2, lst, pq, target) #LV5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly loss analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly(com, df, pq, target):\n",
    "        \n",
    "    def create_merged_cell_lookup(sheet) -> dict:\n",
    "        merged_lookup = {}\n",
    "        for cell_group in sheet.merged_cells.ranges:\n",
    "            min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "            if min_col == max_col:\n",
    "                top_left_cell_value = sheet.cell(row=min_row, column=min_col).value\n",
    "                merged_lookup[str(cell_group)] = top_left_cell_value\n",
    "        return merged_lookup    \n",
    "    \n",
    "    for x,y in com.items():\n",
    "        filter1 = df['EQ_ID'].isin(y)\n",
    "        df2 = df[filter1]\n",
    "        #df = df[(df.LEVEL5_NAME != 'NO MATERIAL')]\n",
    "        #if x == 'a': df2 = df2[(df2.Month == '2022-05') | (df2.Month == '2022-06') | (df2.Month == '2022-07')]\n",
    "        #if x == 'a': df2 =  df2.sort_values(by=['LOG_DATE']).fillna(method='ffill')[(df2.PACKAGE_CLASS_PD == 'MDIP-FP')]\n",
    "        #df2 = df2.groupby(['Month','Process','LEVEL5_NAME','LEVEL6_NAME'], as_index=False).sum()\n",
    "        globals()[x] =pd.crosstab([df2.LEVEL5_NAME,df2.LEVEL6_NAME],df2.Month,values=df2.DURATION_HOUR,aggfunc=sum,rownames=['Level 5','LV6'],colnames=['month'], normalize='columns')\n",
    "        #globals()[x] = pd.crosstab([df2.LEVEL5_NAME],df2.LOG_WEEK,values=df2.DURATION_HOUR,aggfunc=sum,rownames=['Level 5'],colnames=['date'], normalize='columns')\n",
    "\n",
    "        try:\n",
    "            p = globals()[x].loc[['NORMAL PRODUCTION','SPEED LOSS']].sum()\n",
    "            p.name = 'NORMAL PRODUCTION'\n",
    "            p = pd.DataFrame(p)\n",
    "            p = p.transpose()\n",
    "            p.insert(0, \"LV6\", ['NORMAL PRODUCTION'], True)\n",
    "            p = p.set_index('LV6',append=True)\n",
    "            globals()[x] = globals()[x].drop(['NORMAL PRODUCTION','SPEED LOSS'])\n",
    "            globals()[x] = pd.concat([globals()[x], p])\n",
    "            #globals()[x] = globals()[x].append(p)\n",
    "        except KeyError:\n",
    "            pass    \n",
    "\n",
    "        k = pq[pq['EQ_ID'].str.contains(com.get(x)[0])]\n",
    "        tar = target[(target.Device == k.iloc[0]['Device']) & (target.Process == k.iloc[0]['Process'])]\n",
    "        globals()[x] = globals()[x].join(tar.set_index('Level 5')['Loss factor'], on='Level 5')\n",
    "        globals()[x].to_excel(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')\n",
    "\n",
    "        wbook = openpyxl.load_workbook(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')\n",
    "        sheet = wbook[\"Sheet1\"]\n",
    "        lookup = create_merged_cell_lookup(sheet)\n",
    "        cell_group_list = lookup.keys()\n",
    "        for cell_group in cell_group_list:\n",
    "            min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "            sheet.unmerge_cells(str(cell_group))\n",
    "            for row in sheet.iter_rows(min_col=min_col, min_row=min_row, max_col=max_col, max_row=max_row):\n",
    "                for cell in row:\n",
    "                    cell.value = lookup[cell_group]\n",
    "        wbook.save(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly loss analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly(com, df, pq, target):\n",
    "\n",
    "    def create_merged_cell_lookup(sheet) -> dict:\n",
    "        merged_lookup = {}\n",
    "        for cell_group in sheet.merged_cells.ranges:\n",
    "            min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "            if min_col == max_col:\n",
    "                top_left_cell_value = sheet.cell(row=min_row, column=min_col).value\n",
    "                merged_lookup[str(cell_group)] = top_left_cell_value\n",
    "        return merged_lookup        \n",
    "    \n",
    "    for x,y in com.items():\n",
    "        filter1 = df['EQ_ID'].isin(y)\n",
    "        df2 = df[filter1]\n",
    "        #df = df[(df.LEVEL5_NAME != 'NO MATERIAL')]\n",
    "        #if x == 'a': df2 = df2[(df2.Month == '2022-05') | (df2.Month == '2022-06') | (df2.Month == '2022-07')]\n",
    "        #if x == 'a': df2 = df2[(df2.PACKAGE_CLASS_PD == 'MDIP-FP')]    \n",
    "        df2 = df2.groupby(['LOG_WEEK','Process','LEVEL5_NAME','LEVEL6_NAME'], as_index=False).sum()\n",
    "        globals()[x] =pd.crosstab([df2.LEVEL5_NAME,df2.LEVEL6_NAME],df2.LOG_WEEK,values=df2.DURATION_HOUR,aggfunc=sum,rownames=['Level 5','LV6'],colnames=['month'], normalize='columns')\n",
    "        #globals()[x] = pd.crosstab([df2.LEVEL5_NAME],df2.LOG_WEEK,values=df2.DURATION_HOUR,aggfunc=sum,rownames=['Level 5'],colnames=['date'], normalize='columns')\n",
    "\n",
    "        try:\n",
    "            p = globals()[x].loc[['NORMAL PRODUCTION','SPEED LOSS']].sum()\n",
    "            p.name = 'NORMAL PRODUCTION'\n",
    "            p = pd.DataFrame(p)\n",
    "            p = p.transpose()\n",
    "            p.insert(0, \"LV6\", ['NORMAL PRODUCTION'], True)\n",
    "            p = p.set_index('LV6',append=True)\n",
    "            globals()[x] = globals()[x].drop(['NORMAL PRODUCTION','SPEED LOSS'])\n",
    "            globals()[x] = pd.concat([globals()[x], p])\n",
    "            #globals()[x] = globals()[x].append(p)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        k = pq[pq['EQ_ID'].str.contains(com.get(x)[0])]\n",
    "        tar = target[(target.Device == k.iloc[0]['Device']) & (target.Process == k.iloc[0]['Process'])]\n",
    "        globals()[x] = globals()[x].join(tar.set_index('Level 5')['Loss factor'], on='Level 5')\n",
    "        globals()[x].to_excel(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')\n",
    "\n",
    "        wbook = openpyxl.load_workbook(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')\n",
    "        sheet = wbook[\"Sheet1\"]\n",
    "        lookup = create_merged_cell_lookup(sheet)\n",
    "        cell_group_list = lookup.keys()\n",
    "        for cell_group in cell_group_list:\n",
    "            min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "            sheet.unmerge_cells(str(cell_group))\n",
    "            for row in sheet.iter_rows(min_col=min_col, min_row=min_row, max_col=max_col, max_row=max_row):\n",
    "                for cell in row:\n",
    "                    cell.value = lookup[cell_group]\n",
    "        wbook.save(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily loss analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily(com, df, pq, target):\n",
    "\n",
    "    def create_merged_cell_lookup(sheet) -> dict:\n",
    "        merged_lookup = {}\n",
    "        for cell_group in sheet.merged_cells.ranges:\n",
    "            min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "            if min_col == max_col:\n",
    "                top_left_cell_value = sheet.cell(row=min_row, column=min_col).value\n",
    "                merged_lookup[str(cell_group)] = top_left_cell_value\n",
    "        return merged_lookup        \n",
    "\n",
    "    for x,y in com.items():\n",
    "        filter1 = df['EQ_ID'].isin(y)\n",
    "        df2 = df[filter1]\n",
    "        #if x == 'c': df2 = df2[(df2.Month == '2022-07')]\n",
    "        #if x == 'a': df2 = df2[(df2.PACKAGE_CLASS_PD == 'MDIP-FP')]    \n",
    "        df2 = df2.groupby(['LOG_DATE','Process','LEVEL5_NAME','LEVEL6_NAME'], as_index=False).sum()\n",
    "        globals()[x] =pd.crosstab([df2.LEVEL5_NAME,df2.LEVEL6_NAME],df2.LOG_DATE,values=df2.DURATION_HOUR,aggfunc=sum,rownames=['Level 5','LV6'],colnames=['month'], normalize='columns')\n",
    "        #globals()[x] = pd.crosstab([df2.LEVEL5_NAME],df2.LOG_WEEK,values=df2.DURATION_HOUR,aggfunc=sum,rownames=['Level 5'],colnames=['date'], normalize='columns')\n",
    "\n",
    "        try:\n",
    "            p = globals()[x].loc[['NORMAL PRODUCTION','SPEED LOSS']].sum()\n",
    "            p.name = 'NORMAL PRODUCTION'\n",
    "            p = pd.DataFrame(p)\n",
    "            p = p.transpose()\n",
    "            p.insert(0, \"LV6\", ['NORMAL PRODUCTION'], True)\n",
    "            p = p.set_index('LV6',append=True)\n",
    "            globals()[x] = globals()[x].drop(['NORMAL PRODUCTION','SPEED LOSS'])\n",
    "            globals()[x] = pd.concat([globals()[x], p])\n",
    "        except KeyError:\n",
    "            pass    \n",
    "\n",
    "        k = pq[pq['EQ_ID'].str.contains(com.get(x)[0])]\n",
    "        tar = target[(target.Device == k.iloc[0]['Device']) & (target.Process == k.iloc[0]['Process'])]\n",
    "        globals()[x] = globals()[x].join(tar.set_index('Level 5')['Loss factor'], on='Level 5')\n",
    "        globals()[x].to_excel(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')\n",
    "\n",
    "        wbook = openpyxl.load_workbook(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')\n",
    "        sheet = wbook[\"Sheet1\"]\n",
    "        lookup = create_merged_cell_lookup(sheet)\n",
    "        cell_group_list = lookup.keys()\n",
    "        for cell_group in cell_group_list:\n",
    "            min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "            sheet.unmerge_cells(str(cell_group))\n",
    "            for row in sheet.iter_rows(min_col=min_col, min_row=min_row, max_col=max_col, max_row=max_row):\n",
    "                for cell in row:\n",
    "                    cell.value = lookup[cell_group]\n",
    "        wbook.save(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALL(df, pq, target):\n",
    "\n",
    "    def create_merged_cell_lookup(sheet) -> dict:\n",
    "        merged_lookup = {}\n",
    "        for cell_group in sheet.merged_cells.ranges:\n",
    "            min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "            if min_col == max_col:\n",
    "                top_left_cell_value = sheet.cell(row=min_row, column=min_col).value\n",
    "                merged_lookup[str(cell_group)] = top_left_cell_value\n",
    "        return merged_lookup\n",
    "\n",
    "    share = pd.DataFrame({\n",
    "        'Name' : ['FPMold - Hanmi', 'DCBMold - Hanmi', 'TinyLaser marking', 'Tiny 3.0Laser marking', 'TinyAFVI', 'Tiny 3.0AFVI'],\n",
    "        'PACKAGE_CLASS_PD' : ['MDIP-DCB, MDIP-DCBPLUS', 'MDIP-FP', 'MDIP-TINY3', 'MDIP-TINY, MSIP-TINY ', 'MDIP-DCB, MDIP-DCBPLUS, MDIP-FP, MDIP-TINY3', 'MDIP-DCB, MDIP-DCBPLUS, MDIP-FP, MDIP-TINY, MSIP-TINY' ]\n",
    "            })\n",
    "    s = share['Name'].tolist()\n",
    "    \n",
    "    for index, row in pq.iterrows():\n",
    "        k = row\n",
    "        lst = row['EQ_ID'].split(',')\n",
    "        lst = [x.strip(' ') for x in lst]\n",
    "        df2 = df[df['EQ_ID'].isin(lst)]\n",
    "\n",
    "        if row['Device'] + row['Process'] in s:\n",
    "            df2 = df2.sort_values(by=['LOG_DATE'])\n",
    "            df2 = df2.fillna(method='ffill')\n",
    "            l = share.loc[share.Name == row['Device'] + row['Process'],'PACKAGE_CLASS_PD'].tolist()\n",
    "            l = \" \".join(l)\n",
    "            l = l.split(',')\n",
    "            l = [x.strip(' ') for x in l]\n",
    "            for i in l:\n",
    "                df2 = df2[(df2.PACKAGE_CLASS_PD != i)]\n",
    "\n",
    "\n",
    "        #df2 = df2.groupby(['Month','Process','LEVEL5_NAME','LEVEL6_NAME'], as_index=False).sum()\n",
    "        df3 = pd.crosstab([df2.LEVEL5_NAME,df2.LEVEL6_NAME],df2.LOG_WEEK,values=df2.DURATION_HOUR,aggfunc=sum,rownames=['Level 5','LV6'],colnames=['month'], normalize='columns')\n",
    "\n",
    "        try:\n",
    "            p = df3.loc[['NORMAL PRODUCTION','SPEED LOSS']].sum()\n",
    "            p.name = 'NORMAL PRODUCTION'\n",
    "            p = pd.DataFrame(p)\n",
    "            p = p.transpose()\n",
    "            p.insert(0, \"LV6\", ['NORMAL PRODUCTION'], True)\n",
    "            p = p.set_index('LV6',append=True)\n",
    "            df3 = df3.drop(['NORMAL PRODUCTION','SPEED LOSS'])\n",
    "            df3 = pd.concat([df3, p])\n",
    "            #df3 = df3.append(p)\n",
    "        except KeyError:\n",
    "            pass           \n",
    "        \n",
    "        tar = target[(target.Device == row['Device']) & (target.Process == row['Process'])]    \n",
    "        df4 = df3.join(tar.set_index('Level 5')['Loss factor'], on='Level 5')\n",
    "        df4 = df4.round(4)\n",
    "        df4.to_excel(row['Device'] + '_' + row['Process'] + '.xlsx')\n",
    "\n",
    "        wbook = openpyxl.load_workbook(row['Device'] + '_' + row['Process'] + '.xlsx')\n",
    "        sheet = wbook[\"Sheet1\"]\n",
    "        lookup = create_merged_cell_lookup(sheet)\n",
    "        cell_group_list = lookup.keys()\n",
    "        for cell_group in cell_group_list:\n",
    "            min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "            sheet.unmerge_cells(str(cell_group))\n",
    "            for row in sheet.iter_rows(min_col=min_col, min_row=min_row, max_col=max_col, max_row=max_row):\n",
    "                for cell in row:\n",
    "                    cell.value = lookup[cell_group]\n",
    "        wbook.save(k['Device'] + '_' + k['Process'] + '.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LV5 one page summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_page(df, pq, target):\n",
    "    \n",
    "    share = pd.DataFrame({\n",
    "        'Name' : ['FPMold - Hanmi', 'DCBMold - Hanmi', 'TinyLaser marking', 'Tiny 3.0Laser marking', 'TinyAFVI', 'Tiny 3.0AFVI'],\n",
    "        'PACKAGE_CLASS_PD' : ['MDIP-DCB, MDIP-DCBPLUS', 'MDIP-FP', 'MDIP-TINY3', 'MDIP-TINY, MSIP-TINY ', 'MDIP-DCB, MDIP-DCBPLUS, MDIP-FP, MDIP-TINY3', 'MDIP-DCB, MDIP-DCBPLUS, MDIP-FP, MDIP-TINY, MSIP-TINY' ]\n",
    "            })\n",
    "    s = share['Name'].tolist()\n",
    "\n",
    "    def append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None,\n",
    "                           truncate_sheet=False,\n",
    "                           **to_excel_kwargs):\n",
    "\n",
    "\n",
    "        # ignore [engine] parameter if it was passed\n",
    "        if 'engine' in to_excel_kwargs:\n",
    "            to_excel_kwargs.pop('engine')\n",
    "\n",
    "        writer = pd.ExcelWriter(filename, engine='openpyxl', mode='a',if_sheet_exists='overlay')\n",
    "        \n",
    "        try:\n",
    "            FileNotFoundError\n",
    "        except NameError:\n",
    "            FileNotFoundError = IOError\n",
    "            \n",
    "        try:\n",
    "            # try to open an existing workbook\n",
    "            writer.book = load_workbook(filename)\n",
    "\n",
    "            # get the last row in the existing Excel sheet\n",
    "            # if it was not specified explicitly\n",
    "            if startrow is None and sheet_name in writer.book.sheetnames:\n",
    "                startrow = writer.book[sheet_name].max_row\n",
    "\n",
    "            # truncate sheet\n",
    "            if truncate_sheet and sheet_name in writer.book.sheetnames:\n",
    "                # index of [sheet_name] sheet\n",
    "                idx = writer.book.sheetnames.index(sheet_name)\n",
    "                # remove [sheet_name]\n",
    "                writer.book.remove(writer.book.worksheets[idx])\n",
    "                # create an empty sheet [sheet_name] using old index\n",
    "                writer.book.create_sheet(sheet_name, idx)\n",
    "\n",
    "            # copy existing sheets\n",
    "            writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n",
    "        except FileNotFoundError:\n",
    "            # file does not exist yet, we will create it\n",
    "            pass\n",
    "            \n",
    "        if startrow is None:\n",
    "            startrow = 0\n",
    "        #with pd.ExcelWriter('recent_OEE3.xlsx', mode='a') as writer:\n",
    "        df.to_excel(writer, sheet_name, startrow=startrow, **to_excel_kwargs)\n",
    "        \n",
    "        writer.save()\n",
    "        \n",
    "    filename = r'C:\\Users\\ParkPaul\\Documents\\Python\\recent_OEE3.xlsx'\n",
    "\n",
    "    for index, row in pq.iterrows():\n",
    "        k = row\n",
    "        lst = row['EQ_ID'].split(',')\n",
    "        lst = [x.strip(' ') for x in lst]\n",
    "        df2 = df[df['EQ_ID'].isin(lst)]\n",
    "\n",
    "        if row['Device'] + row['Process'] in s:\n",
    "            df2 = df2.sort_values(by=['LOG_DATE'])\n",
    "            df2 = df2.fillna(method='ffill')\n",
    "            l = share.loc[share.Name == row['Device'] + row['Process'],'PACKAGE_CLASS_PD'].tolist()\n",
    "            l = \" \".join(l)\n",
    "            l = l.split(',')\n",
    "            l = [x.strip(' ') for x in l]\n",
    "            for i in l:\n",
    "                df2 = df2[(df2.PACKAGE_CLASS_PD != i)]\n",
    "\n",
    "        df2 = pd.crosstab([df2.LEVEL5_NAME],df2.Month,values=df2.DURATION_HOUR,aggfunc=sum,rownames=['Level 5'],colnames=['month'], normalize='columns')\n",
    "\n",
    "        try:\n",
    "            p = df2.loc[['NORMAL PRODUCTION','SPEED LOSS']].sum()\n",
    "            p.name = 'NORMAL PRODUCTION'\n",
    "            df2 = df2.drop(['NORMAL PRODUCTION','SPEED LOSS'])\n",
    "            pd.concat([df2, p.transpose()])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        df2['Average'] = df2.mean(axis=1)\n",
    "        tar = target[(target.Device == row['Device']) & (target.Process == row['Process'])]\n",
    "        df3 = df2.join(tar.set_index('Level 5')['Loss factor'], on='Level 5')\n",
    "        df3 = df3.reset_index()\n",
    "        df3 = df3.rename(columns={'Level 5': row['Device'] + \"_\" + row['Process']})\n",
    "        df3 = df3.round(4)\n",
    "        df3 = df3.sort_values(by='Loss factor', ascending=False)\n",
    "\n",
    "\n",
    "        append_df_to_excel(filename, df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_page(df, pq, target) # LV5 OF all process and one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EQ_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EQ_ID(df2, lst, pq, target):\n",
    "    def create_merged_cell_lookup(sheet) -> dict:\n",
    "        merged_lookup = {}\n",
    "        for cell_group in sheet.merged_cells.ranges:\n",
    "            min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "            if min_col == max_col:\n",
    "                top_left_cell_value = sheet.cell(row=min_row, column=min_col).value\n",
    "                merged_lookup[str(cell_group)] = top_left_cell_value\n",
    "        return merged_lookup\n",
    "\n",
    "    df3 = df2.pivot_table(index=['LEVEL5_NAME'], columns = ['EQ_ID','Month'], values='DURATION_HOUR', aggfunc={'sum'}).apply(lambda y:y/float(y.sum())).round(2)\n",
    "    \n",
    "    try:\n",
    "        p = df3.loc[['NORMAL PRODUCTION','SPEED LOSS']].sum()\n",
    "        p.name = 'NORMAL PRODUCTION'\n",
    "        df3 = df3.drop(['NORMAL PRODUCTION','SPEED LOSS'])\n",
    "        df3 = df3.append(p.transpose())\n",
    "        #pd.concat([df3, p.transpose()])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    k = pq[pq['EQ_ID'].str.contains(lst[0])]\n",
    "    tar = target[(target.Device == k.iloc[0]['Device']) & (target.Process == k.iloc[0]['Process'])]\n",
    "    df3 = df3.reset_index()\n",
    "    df3 = df3.rename(columns={'LEVEL5_NAME': 'Level 5'})\n",
    "    df3 = df3.set_index('Level 5')\n",
    "    tar = pd.DataFrame(tar.set_index('Level 5')['Loss factor'])\n",
    "    tar.columns = pd.MultiIndex.from_product([['a'],['b'], tar.columns])\n",
    "    df3 = df3.join(tar, on='Level 5')\n",
    "    df3 = df3.replace(np.nan, 0)\n",
    "    col = df3.columns.tolist()\n",
    "    col = col[-1:] + col[:-1]\n",
    "    df3 = df3[col]\n",
    "    df3.sort_values(by=[('a', 'b', 'Loss factor')], inplace=True, ascending=False)\n",
    "    df3.to_excel(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')\n",
    "\n",
    "    wbook = openpyxl.load_workbook(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')\n",
    "    sheet = wbook[\"Sheet1\"]\n",
    "    lookup = create_merged_cell_lookup(sheet)\n",
    "    cell_group_list = lookup.keys()\n",
    "    for cell_group in cell_group_list:\n",
    "        min_col, min_row, max_col, max_row = range_boundaries(str(cell_group))\n",
    "        sheet.unmerge_cells(str(cell_group))\n",
    "        for row in sheet.iter_rows(min_col=min_col, min_row=min_row, max_col=max_col, max_row=max_row):\n",
    "            for cell in row:\n",
    "                cell.value = lookup[cell_group]\n",
    "    wbook.save(k.iloc[0]['Device'] + '_' + k.iloc[0]['Process'] + '.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
